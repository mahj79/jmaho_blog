---
title: 'Neural Networks and Machine Learning (Part 2)'
date: '2024-07-22'
---

We wrapped up the first part of this series focusing on how a neural network connects and interacts with each of its respective layers. To take a step back though let’s recap and talk about the big picture, how a neural network is relevant to machine learning and what it ultimately accomplishes. We talked about how machine learning is basically a program that continuously trains itself on data to produce more accurate outputs based on its intended purpose. In the example of the puzzle we gave in part 1, the expectation would be that by the time the program has run a 1000 times it should be able to identify each piece accurately and quickly. This is done by leveraging the neural network inside the program to make accurate decisions on which piece is which. To get to those accurate decisions though, we have to train our program. This is done by adjusting the weights and biases associated with the neurons from layer to layer, which is the main subject of this part.  

When we say weights and biases, this refers to the importance we put on layers of our neural network based on what we tell the program. The weights piece focuses on the connections we have between layers. Without going too much into the math, we can think of the weight we apply to a connection between two layers as us telling the program hot or cold. If hot is represented by the number 1 and cold is the number 0, the hotter we make a connection the more we influence the program to look for a specific element. We’re going to skip over the details but if you are interested in learning more about the actual math on how this works, the sigmoid function is how weights are applied on a log curve. The resources I linked in the first part all cover this in detail plus some.

If you recall from part one, we compared layers to filters in a neural network. A layer will have multiple neurons that essentially act as a number holder for an evaluation technique when assessed against all the other neurons in the layer. It will provide relevant information to the other layers using some advanced math (sigmoid function plus some).Now let’s go back to our puzzle example. If the goal of our program is to solve a puzzle and we want the program to start looking for corner puzzle pieces, we would put an emphasis on having connections that have flat sides. To apply this logic we would tell our program at the first layer if you see something that looks like a flat side, it may be a corner piece and to pass that info along to the next layer. We do this by putting more weight on the connections that look for flat sides. 

You may think well why don’t we make it yes or no if it has a flat side and pass it along? Well that’s an option if we were only looking for corner pieces but in this case there will be other pieces of the puzzle that we’ll need to identify such as a middle piece with no flat sides. This is accounted for by organizing our neurons to communicate based on how we assign our layers to look for things. Take a look at the graphics below. In the first graphic we have that first layer that has received a corner piece. The first layer is looking for flat sides and we’ve put weight on flat sides to basically tell this layer, hey we have some flat sides, pass this info along to the next layer which fires the highlighted neurons in red. In the second graphic, we pass in a middle piece with no flat sides. This gets passed along to the next layer firing different neurons in blue. 

![Neural Network Drawing Red Weights](/images/Neural_Network_Drawing_Red_Weights.jpg "Neural Network Drawing Red Weights")
Figure (Neurons Firing in Red)

![Neural Network Drawing Blue Weights](/images/Neural_Network_Drawing_Blue_Weights.jpg "Neural Network Drawing Blue Weights")
Figure (Neurons Firing in Blue)

If it still feels a bit confusing on what a weight is, the other analogy to think of is a two sided balance scale. If you put more weight on one side, the scale will tip that way. If you put more weight on the other side of the scale then vice versa. We apply this logic as a way to help train our program on how it should think. 

Ok so we’ve covered weights quite a bit, but Maho you mentioned something about biases in the beginning and all this weight talk sounds an awful lot like biases? Well you’re sort of right. Weights and biases are similar in that we are essentially telling the program what it should be caring about. So what is a bias? A bias is how we leverage the neurons in a layer to say what or what not to look for. The bias helps us get closer to that 1 or 0 that tells our layers this is what we should be caring about. From a mathematical perspective it acts sort of like a negative effect. 

Weights are the connections between layers that pass along information from layer to layer based on what we want each layer to focus on. Bias is how we fine tune the neurons in each layer to then pass along the information from layer to layer. If we go back to our puzzle example, we used an example of looking for a corner piece. That first layer is checking to see if the piece has a flat side. Well how that layer determines if a side is flat is effectively the neurons with appropriate bias at play. We tell the neurons in the layer to have a certain bias using math to look for sides that are flat. Similar to telling weight if it’s hot or cold, bias uses a similar method to check if something aligns to the expected outcome. If the piece had a flat side, there would be numbers assigned to where the piece was flat close to 1, where if there was curvature or nothing at all suggesting not part of the piece it would trend negative closer to 0. Use the below graphic to help visualize this.  

![Neural Network Drawing Puzzle Bias](/images/Neural_Network_Drawing_Puzzle_Bias.jpg "Neural Network Drawing Puzzle Bias")
Figure (Bias Visualized with Numbers)

The bias plays a critical part in training our neural network / machine learning program as this is what we can continuously fine tune to make predictions more accurate. The closer a bias has to 1, the more likely that neuron is identifying the correct thing it has been told to look for. The further away from 1 or you can think of as closer to 0, the program is identifying something that is not what it is supposed to be looking for. The next part we’ll cover how to further train and enhance the accuracy of our models but as a sneak peak, know that it has to do with adjusting biases and weights to tell our program when it is or isn’t correct. Keep in mind that bias and weight all feed into some more advanced math, but the key point is that this is how we train neural networks / machine learning programs over time to further refine accuracy. 

So let’s summarize what we’ve learned up to this point. A neural network is the foundational piece of how machine learning works, which is designed to replicate how neurons in our brain works. The neural network is composed of layers, with a number of neurons in each layer. Each layer is designed to perform a very specific function based on what the neurons in each layer are programmed to look for. Each layer is connected to each other and leverages weights to inform layer to layer what the program is looking for. Bias is how we tell our neurons if a neuron is or isn’t looking at the right thing. Leveraging weights and biases we are able to train our program to be more accurate as it communicates from layer to layer. 

Hopefully you’ve been able to follow along up to this point and if something isn’t making sense, please let me know! Also feel free to check out the material I linked in the first post as that goes much deeper into specifics and contains a lot of helpful supplementary information. The next post will dive into the meat of how a program trains itself over time to produce more accurate outcomes.
