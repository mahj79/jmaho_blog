---
title: 'Neural Networks and Machine Learning (Part 3)'
date: '2024-08-13'
---

Alright folks, if you’ve made it to the third and final part of this series let me start by saying a big thank you! This is mostly for fun and a way to learn new things but I appreciate you taking your time to go along this journey with me nonetheless. That being said, this will be the last part of the series on machine learning and neural networks. Let’s do one quick recap of what we’ve learned through the first two posts as refresher. A neural network is the crux of how machine learning works. The neural network is composed of layers, with a number of neurons in each layer. Each layer is designed to perform a very specific function. Each layer is connected to one another and leverages weights to inform each other based on what the program is looking for. Bias is how we tell our neurons if it is or isn’t looking at the right thing. By leveraging weights and biases, we are able to train our program for more accurate outputs.

For this third part, we’re going to be focusing on how adjusting our theoretical knobs can fine tune our program to produce more accurate answers. In the example we’ve been using, our program is intended to solve a puzzle by correctly identifying each piece of the puzzle.The aforementioned knobs refer to the weights and biases we discussed in Part 2. So as we look to mature our program to further improve the performance of how and which piece of the puzzle is identified, we can fine tune the accuracy by slightly adjusting the weights and biases. To first determine the accuracy of our program, we first run it a few times and evaluate how successful the program was at identifying the correct options. Let’s say we run the program 10 times and 8 times it correctly identifies the correct answers. We look at the results and identify some of the potential areas that may be leading to incorrect answers, make some minor adjustments to weights and biases, and then run it 100 times producing a correct answer 86 times. That's an improvement! Once again we dive into the results and identify some more areas that may be leading to incorrect answers, run it 1000 times producing a correct answer 900 times. Once again we see improvement telling us we are making progress on improving the accuracy of our program. Conceptually this is how we fine tune our program to improve over time. Note that it is not guaranteed to show improvement every time you make adjustments, sometimes you can decrease the accuracy. This is why it’s important to take important notes on what changes and adjustments you make to your program before rerunning the program. 

Up to this point we have avoided talking too much about how math is applied from a machine learning and neural network standpoint. Unfortunately it could only be avoided for so long and it is now time to talk a little about how adjusting weights and biases leverages math to fine tune our program. To understand how the math works, we start with what a very simple cost function is. Cost functions are not exclusive to machine learning, in fact they are very common in most facets of business. The function is defined as C(x) = F + V(x), where F is the fixed cost for running the program, V(x) is the variable cost per unit of the program, and C(x) is the total cost to produce an answer. In machine learning it’s slightly more complex, however at a high level this is how the function works. Diving into how this function applies to what we’ve learned thus far, F refers to the fixed cost representing the number of pieces in our puzzle that is an input into the program. The variable cost, V(x) refers to what we adjust such as the weights and biases. Then finally C(x) is the output we receive from running our model. The purpose of this cost function is to continuously measure the difference between the predicted outputs of the model and the actual target values. The predicted outputs being what the program believes the puzzle piece should be and where it should go and the actual target values which are what the puzzle piece actually is and where it truly belongs. The main goal of training a program like this is to minimize the cost function, and over time improving the accuracy.

So we have this cost function thing and we sort of know that adjusting weights and biases is important to improving accuracy, but it still might not be 100% clear how this all ties together. Well there’s still a little bit more math stuff we have to touch on to bring this conceptually all together. When we talk about minimizing the cost function to improve accuracy over time, we have to talk about gradient descent. Gradient Descent is how we optimize our model to minimize the cost function in machine learning and neural networks. It's one of the most common methods for training machine learning models. Some of the key concepts are summarized below for you to further understand why this is relevant, including some examples from our puzzle use case.

- **Objective:** As noted, the main goal of gradient descent is to find the values of the program’s parameters (like weights and biases) that minimize the cost function, thus improving the accuracy of the model's predictions.
- **Gradient:** The gradient is a vector of partial derivatives, (the V(x) we noted earlier) of the cost function with respect to the model parameters. If you’re not familiar with derivatives and vectors, it’s basically fancy math speak for how change is measured and in this case specifically how we calculate the low point we’re looking for. The gradient points in the direction of the steepest increase in the cost function. However, since we’re looking for the low point because that is how we get more accurate, we would move in the opposite direction (down the gradient), so the program decreases the cost.
- **Learning Rate:** This is a parameter that determines the size of the steps our cost function takes towards reaching the minimum. A small learning rate means the function takes small steps, leading to slow convergence but more precise adjustments. A large learning rate can speed up convergence but risks overshooting the minimum or missing completely.
- **Steps of Gradient Descent:**
    - **Initialize Parameters:** Start with some initial values for the model's parameters, often chosen randomly. In our puzzle case it may mean something like “hey look for the color green or blue, but once it identifies a bunch of red, the program will say yup no green or blue here so we know in the future that’s not what we want”
    - **Compute Predictions:** Use the current parameters to make predictions on the training data. This would be looking at the parameters we set and then looking at how we defined our weights and biases to make an educated guess or calculation on how we think our program will perform
    - **Calculate Cost:** Compute the cost function to see how far off the predictions are from the actual values. Basically a comparison of how accurately were pieces of the puzzle identified 
    - **Compute Gradients:** Calculate the gradients of the cost function with respect to each parameter. This involves taking the partial derivative of the cost function with respect to each parameter. Think of this as how much did our weights and biases contribute to the program producing a right answer
    - **Update Parameters:** Adjust the parameters in the opposite direction of the gradient by a step size determined by the learning rate. Take what we’ve learned from running the program and adjust those knobs to produce a more accurate answer the next time the program runs!

To help visualize all this, imagine that you’re standing on top of a mountain. You’re not quite sure how you got there but you know that at the bottom of the mountain there is a nice picnic waiting for you if you can reach it. Well obviously that sounds lovely, however you’re not quite sure how to get down there because there’s plenty of paths leading down the mountain. There’s a few factors you have to consider as well such as time it takes to get down the mountain, least amount of steps, safety of the paths, and so on. After some thought you decide you want to aim for the quickest way down the mountain as possible. You make your way down the path you think is the fastest but ultimately you end going down too steep of a decline and roll your ankle. Eventually you make it all the way down and enjoy your picnic but not without some bumps and bruises. This essentially is the equivalent of our program looking for the right puzzle piece but messing up a few times. Well let’s say for a year straight, every day you keep ending up at the top of that mountain with a hankering for a picnic. Over time you will continue to find your way down that mountain, eventually figuring out the most optimal way down to your picnic. Congrats! Conceptually, that is gradient descent and how machine learning and neural networks figure out how to produce accurate answers.

Ok so let’s wrap it all up and put a bow on this series! A neural network is the foundational piece of how machine learning works, which is designed to replicate how neurons in our brain work. The neural network is composed of layers, with a number of neurons in each layer. Each layer is designed to perform a very specific function based on what the neurons in each layer are programmed to look for. Each layer is connected to one another and leverages weights to inform, layer to layer, what the program is looking for. Bias is how we tell our neurons if it is or isn’t looking at the right thing. By leveraging weights and biases, we are able to train our program to be more accurate as it communicates through the layers. Weights and biases are inputs into a cost function. A cost function is the math behind our program to calculate how accurate our program is. The program’s cost functions use gradient descent to find the most important weights and biases to fine tune our program for accuracy. By adjusting weights and biases we improve the gradient descent and refine our program. 

All of this together describes the foundational parts of how machine learning works. If you found these posts interesting, I would highly encourage checking out some of the resources I linked in the first post. I may dive into some other related topics in the future, but that’ll do it for at least this series. Hopefully you learned something from these posts and if you have any feedback, please share. 
