---
title: 'Neural Networks and Machine Learning (Part 3)'
date: '2024-08-13'
---

Alright folks, if you’ve made it to the third and final part of this series let me start by saying a big thank you! This is mostly for fun and a way to learn new things but I appreciate you taking your time to go along this journey with me nonetheless. That being said, this will be the last part of the series on machine learning and neural networks. Let’s do one quick recap of what we’ve learned through the first two posts as refresher. A neural network is the crux of how machine learning works. The neural network is composed of layers, with a number of neurons in each layer. Each layer is designed to perform a very specific function. Each layer is connected to one another and leverages weights to inform each other based on what the program is looking for. Bias is how we tell our neurons if it is or isn’t looking at the right thing. By leveraging weights and biases, we are able to train our program for more accurate outputs.

For this third part, we’re going to be focusing on how adjusting our theoretical knobs can fine tune our program to produce more accurate answers. In the example we’ve been using, our program is intended to solve a puzzle by correctly identifying each piece of the puzzle.The aforementioned knobs refer to the weights and biases we discussed in Part 2. So as we look to mature our program to further improve the performance of how and which piece of the puzzle is identified, we can fine tune the accuracy by slightly adjusting the weights and biases.  
